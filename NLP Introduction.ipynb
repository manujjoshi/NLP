{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NLP Introduction`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Interactive voice response](https://en.wikipedia.org/wiki/Interactive_voice_response)\n",
    "\n",
    "### 2. For developers and data scientist it become very easy to build a model which is domain specific (Digital Personal Assistant)\n",
    "\n",
    "### 3. Collecting raw test is quite easy comaparative to images\n",
    "\n",
    "## `NLP vs NLG vs NLU`\n",
    "\n",
    "### 4. NLP = NLG + NLU\n",
    "- NLP is **Natural Language Processing**\n",
    "- NLG is **Natural Language Generation**, is basically used for generating text, generating relationship, generating entities. **Here we are interested in generating the text**.\n",
    "- NLU is **Natural Language Understanding**, here we are more interested in understanding the **meaning of the text**. eg. Sentiment detection, topic classification. Determining the sematic meaning of the text.\n",
    "- **In industry NLP and NLU are widely used, NLG is something very risky because we are generating a text**.\n",
    "\n",
    "### Use case:\n",
    "1. **Machine Translation**\n",
    "2. **Sentiment/Opinion Analysis**\n",
    "3. **Spam Detection**\n",
    "4. **Digital personal Assistant**\n",
    "5. **InShorts (NLP + NLU)** : Text summarization\n",
    "\n",
    "### Commonly used terms:\n",
    "1. Corpora: group of corpus creates a corpora\n",
    "2. Corpus: group of document creates a corpus\n",
    "3. Document: group of tokens creates a document\n",
    "4. Token: single word is a token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tokenization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\manuj\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\manuj\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\manuj\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manuj\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\manuj\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### punkt is framework of nltk, helps in tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing global pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The novel virus was first identified from an outbreak in the Chinese city of Wuhan in December 2019, and attempts to contain it there failed, allowing it to spread across the globe. The World Health Organization (WHO) declared a Public Health Emergency of International Concern on 30 January 2020 and a pandemic on 11 March 2020. As of 14 January 2022, the pandemic had caused more than 321 million cases and 5.52 million deaths, making it one of the deadliest in history.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing global pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The novel virus was first identified from an outbreak in the Chinese city of Wuhan in December 2019, and attempts to contain it there failed, allowing it to spread across the globe. The World Health Organization (WHO) declared a Public Health Emergency of International Concern on 30 January 2020 and a pandemic on 11 March 2020. As of 14 January 2022, the pandemic had caused more than 321 million cases and 5.52 million deaths, making it one of the deadliest in history.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence tokenizer => breaking a given document into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "\n",
      "\n",
      "['The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing global pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).', 'The novel virus was first identified from an outbreak in the Chinese city of Wuhan in December 2019, and attempts to contain it there failed, allowing it to spread across the globe.', 'The World Health Organization (WHO) declared a Public Health Emergency of International Concern on 30 January 2020 and a pandemic on 11 March 2020.', 'As of 14 January 2022, the pandemic had caused more than 321 million cases and 5.52 million deaths, making it one of the deadliest in history.']\n",
      "\n",
      "\n",
      "The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing global pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences.__len__())\n",
    "print()\n",
    "print()\n",
    "print(sentences)\n",
    "print()\n",
    "print()\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "\n",
      "\n",
      "['The', 'COVID-19', 'pandemic', ',', 'also', 'known', 'as', 'the', 'coronavirus', 'pandemic', ',', 'is', 'an', 'ongoing', 'global', 'pandemic', 'of', 'coronavirus', 'disease', '2019', '(', 'COVID-19', ')', 'caused', 'by', 'severe', 'acute', 'respiratory', 'syndrome', 'coronavirus', '2', '(', 'SARS-CoV-2', ')', '.', 'The', 'novel', 'virus', 'was', 'first', 'identified', 'from', 'an', 'outbreak', 'in', 'the', 'Chinese', 'city', 'of', 'Wuhan', 'in', 'December', '2019', ',', 'and', 'attempts', 'to', 'contain', 'it', 'there', 'failed', ',', 'allowing', 'it', 'to', 'spread', 'across', 'the', 'globe', '.', 'The', 'World', 'Health', 'Organization', '(', 'WHO', ')', 'declared', 'a', 'Public', 'Health', 'Emergency', 'of', 'International', 'Concern', 'on', '30', 'January', '2020', 'and', 'a', 'pandemic', 'on', '11', 'March', '2020', '.', 'As', 'of', '14', 'January', '2022', ',', 'the', 'pandemic', 'had', 'caused', 'more', 'than', '321', 'million', 'cases', 'and', '5.52', 'million', 'deaths', ',', 'making', 'it', 'one', 'of', 'the', 'deadliest', 'in', 'history', '.']\n",
      "\n",
      "\n",
      "['The', 'COVID-19', 'pandemic', ',', 'also']\n"
     ]
    }
   ],
   "source": [
    "words_1 = nltk.word_tokenize(text)\n",
    "print(words_1.__len__())\n",
    "print()\n",
    "print()\n",
    "print(words_1)\n",
    "print()\n",
    "print()\n",
    "print(words_1[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tweet tokenizer for tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ['@innomatics: Deep Learning course starting from January','@Manuj: Innomatics is a great institute for data science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@', 'innomatics', ':', 'Deep', 'Learning', 'course', 'starting', 'from', 'January'], ['@', 'Manuj', ':', 'Innomatics', 'is', 'a', 'great', 'institute', 'for', 'data', 'science']]\n"
     ]
    }
   ],
   "source": [
    "words = [nltk.word_tokenize(i) for i in tweets] \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[':', 'Deep', 'Learning', 'course', 'starting', 'from', 'January'], [':', 'Innomatics', 'is', 'a', 'great', 'institute', 'for', 'data', 'science']]\n"
     ]
    }
   ],
   "source": [
    "words = [nltk.TweetTokenizer(strip_handles=True).tokenize(i) for i in tweets]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @innomatics/manuj went off\n",
    "### we have the content which we were looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex tokenizer => to extract any pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Jio smart phone no : 1800-565-545\n",
    "Jio helpline no : 1800-256-254,serives are avilable from 9am to 12pm\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jio smart phone no : 1800-565-545\\nJio helpline no : 1800-256-254,serives are avilable from 9am to 12pm\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = ['\\d\\d\\d\\d-\\d\\d\\d-\\d\\d\\d','\\d{1,2}\\w[am|pm]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1800-565-545', '1800-256-254'], ['9am', '12pm']]\n"
     ]
    }
   ],
   "source": [
    "regex_words = [nltk.regexp_tokenize(text,pattern=i) for i in pattern]\n",
    "print(regex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paranthesis tokenizer => will keep the entire parenthesis with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My phone number is (+91)9856423789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'phone', 'number', 'is', '(+91)', '9856423789']\n"
     ]
    }
   ],
   "source": [
    "parenthesis = nltk.SExprTokenizer().tokenize(text)\n",
    "print(parenthesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `END ---------------------------------`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
