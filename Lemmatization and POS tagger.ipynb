{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Lemmatization and POS tagger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\manuj\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\manuj\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manuj\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: click in c:\\users\\manuj\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\manuj\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as WN\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('run.n.05'),\n",
       " Synset('run.n.07'),\n",
       " Synset('running.n.03'),\n",
       " Synset('running.n.04'),\n",
       " Synset('track.n.11'),\n",
       " Synset('run.v.01'),\n",
       " Synset('scat.v.01'),\n",
       " Synset('run.v.03'),\n",
       " Synset('operate.v.01'),\n",
       " Synset('run.v.05'),\n",
       " Synset('run.v.06'),\n",
       " Synset('function.v.01'),\n",
       " Synset('range.v.01'),\n",
       " Synset('campaign.v.01'),\n",
       " Synset('play.v.18'),\n",
       " Synset('run.v.11'),\n",
       " Synset('tend.v.01'),\n",
       " Synset('run.v.13'),\n",
       " Synset('run.v.14'),\n",
       " Synset('run.v.15'),\n",
       " Synset('run.v.16'),\n",
       " Synset('prevail.v.03'),\n",
       " Synset('run.v.18'),\n",
       " Synset('run.v.19'),\n",
       " Synset('carry.v.15'),\n",
       " Synset('run.v.21'),\n",
       " Synset('guide.v.05'),\n",
       " Synset('run.v.23'),\n",
       " Synset('run.v.24'),\n",
       " Synset('run.v.25'),\n",
       " Synset('run.v.26'),\n",
       " Synset('run.v.27'),\n",
       " Synset('run.v.28'),\n",
       " Synset('run.v.29'),\n",
       " Synset('run.v.30'),\n",
       " Synset('run.v.31'),\n",
       " Synset('run.v.32'),\n",
       " Synset('run.v.33'),\n",
       " Synset('run.v.34'),\n",
       " Synset('ply.v.03'),\n",
       " Synset('hunt.v.01'),\n",
       " Synset('race.v.02'),\n",
       " Synset('move.v.13'),\n",
       " Synset('melt.v.01'),\n",
       " Synset('ladder.v.01'),\n",
       " Synset('run.v.41'),\n",
       " Synset('running.a.01'),\n",
       " Synset('running.s.02'),\n",
       " Synset('running.a.03'),\n",
       " Synset('running.a.04'),\n",
       " Synset('linear.s.05'),\n",
       " Synset('running.s.06')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WN.synsets('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'measured lengthwise'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WN.synset('linear.s.05').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come unraveled or undone as if by snagging'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WN.synset('ladder.v.01').definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### somebody is updating at the backend therefore versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Her nylons were running']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WN.synset('ladder.v.01').examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer names / synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syn : ['helping', 'portion', 'serving']\n",
      "syn : ['help', 'assist', 'aid']\n",
      "syn : ['help', 'aid']\n",
      "syn : ['help', 'facilitate']\n",
      "syn : ['help_oneself', 'help']\n",
      "syn : ['serve', 'help']\n",
      "syn : ['help']\n",
      "syn : ['avail', 'help']\n",
      "syn : ['help']\n"
     ]
    }
   ],
   "source": [
    "for i in WN.synsets('helping'):\n",
    "    print('syn :',i.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('helping.n.01'),\n",
       " Synset('help.v.01'),\n",
       " Synset('help.v.02'),\n",
       " Synset('help.v.03'),\n",
       " Synset('help_oneself.v.01'),\n",
       " Synset('serve.v.05'),\n",
       " Synset('help.v.06'),\n",
       " Synset('avail.v.03'),\n",
       " Synset('help.v.08')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WN.synsets('helping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'the cat is sitting with two puppies and playing with them since morning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'sitting', 'with', 'two', 'puppies', 'and', 'playing', 'with', 'them', 'since', 'morning']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sent)\n",
    "print(words)\n",
    "lemma = []\n",
    "for i in words:\n",
    "    val = nltk.stem.WordNetLemmatizer().lemmatize(i)\n",
    "    lemma.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'sitting', 'with', 'two', 'puppy', 'and', 'playing', 'with', 'them', 'since', 'morning']\n"
     ]
    }
   ],
   "source": [
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization using verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'be', 'sit', 'with', 'two', 'puppies', 'and', 'play', 'with', 'them', 'since', 'morning']\n"
     ]
    }
   ],
   "source": [
    "lemma_using_verb = [lemmatizer.lemmatize(i,'v') for i in nltk.word_tokenize(sent)]\n",
    "print(lemma_using_verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### focusing on tenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `POS tagger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\manuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "## nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(sent)\n",
    "tags = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), ('two', 'CD'), ('puppies', 'NNS'), ('and', 'CC'), ('playing', 'VBG'), ('with', 'IN'), ('them', 'PRP'), ('since', 'IN'), ('morning', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the grammer tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('IN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verb and noun contains the real meaning\n",
    "### there is a corpus based on which the above grammatical features were learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\manuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_corpus = nltk.corpus.brown.words()\n",
    "words_corpus.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.Text(item.lower() for item in words_corpus[:1000])\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "court\n"
     ]
    }
   ],
   "source": [
    "tokens.similar('county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term outgoing\n"
     ]
    }
   ],
   "source": [
    "tokens.similar('grand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jurors mayor petition\n"
     ]
    }
   ],
   "source": [
    "tokens.similar('jury')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measures the similarity between tokens\n",
    "## Universal tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\manuj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('Fulton', 'NOUN'),\n",
       " ('County', 'NOUN'),\n",
       " ('Grand', 'ADJ'),\n",
       " ('Jury', 'NOUN'),\n",
       " ('said', 'VERB'),\n",
       " ('Friday', 'NOUN'),\n",
       " ('an', 'DET'),\n",
       " ('investigation', 'NOUN'),\n",
       " ('of', 'ADP')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.corpus.brown.tagged_words(tagset='universal')\n",
    "tagged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('the', 'DET'), 7), (('of', 'ADP'), 5), (('The', 'DET'), 3), (('election', 'NOUN'), 3), (('``', '.'), 3), ((\"''\", '.'), 3), (('in', 'ADP'), 3), (('which', 'DET'), 3), (('Fulton', 'NOUN'), 2), (('said', 'VERB'), 2), (('primary', 'NOUN'), 2), (('that', 'ADP'), 2), (('irregularities', 'NOUN'), 2), (('.', '.'), 2), (('jury', 'NOUN'), 2), (('City', 'NOUN'), 2), ((',', '.'), 2), (('had', 'VERB'), 2), (('was', 'VERB'), 2), (('by', 'ADP'), 2), (('County', 'NOUN'), 1), (('Grand', 'ADJ'), 1), (('Jury', 'NOUN'), 1), (('Friday', 'NOUN'), 1), (('an', 'DET'), 1), (('investigation', 'NOUN'), 1), ((\"Atlanta's\", 'NOUN'), 1), (('recent', 'ADJ'), 1), (('produced', 'VERB'), 1), (('no', 'DET'), 1), (('evidence', 'NOUN'), 1), (('any', 'DET'), 1), (('took', 'VERB'), 1), (('place', 'NOUN'), 1), (('further', 'ADV'), 1), (('term-end', 'NOUN'), 1), (('presentments', 'NOUN'), 1), (('Executive', 'ADJ'), 1), (('Committee', 'NOUN'), 1), (('over-all', 'ADJ'), 1), (('charge', 'NOUN'), 1), (('deserves', 'VERB'), 1), (('praise', 'NOUN'), 1), (('and', 'CONJ'), 1), (('thanks', 'NOUN'), 1), (('Atlanta', 'NOUN'), 1), (('for', 'ADP'), 1), (('manner', 'NOUN'), 1), (('conducted', 'VERB'), 1), (('September-October', 'NOUN'), 1), (('term', 'NOUN'), 1), (('been', 'VERB'), 1), (('charged', 'VERB'), 1), (('Superior', 'ADJ'), 1), (('Court', 'NOUN'), 1), (('Judge', 'NOUN'), 1), (('Durwood', 'NOUN'), 1), (('Pye', 'NOUN'), 1), (('to', 'PRT'), 1), (('investigate', 'VERB'), 1), (('reports', 'NOUN'), 1), (('possible', 'ADJ'), 1), (('hard-fought', 'ADJ'), 1), (('won', 'VERB'), 1), (('Mayor-nominate', 'NOUN'), 1), (('Ivan', 'NOUN'), 1)]\n"
     ]
    }
   ],
   "source": [
    "tagged_freq = nltk.FreqDist(tagged[:100])\n",
    "print(tagged_freq.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counting grammatical features in a given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'IN': 3, 'NN': 2, 'VBG': 2, 'DT': 1, 'VBZ': 1, 'CD': 1, 'NNS': 1, 'CC': 1, 'PRP': 1})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "tags_freq = nltk.FreqDist(item for (word,item) in tags)\n",
    "tags_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Ngrams`\n",
    "- different type of tokens and it's accociated tokens\n",
    "#### Types:\n",
    "1. Unigrams: one token per word\n",
    "2. Bigram: two tokens together\n",
    "3. Trigram: three words together\n",
    "4. Qudgram: four words together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = ngrams([i for i in nltk.word_tokenize(sent)],1)\n",
    "bigram = ngrams([i for i in nltk.word_tokenize(sent)],2)\n",
    "trigram = ngrams([i for i in nltk.word_tokenize(sent)],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the',), ('cat',), ('is',), ('sitting',), ('with',), ('two',), ('puppies',), ('and',), ('playing',), ('with',), ('them',), ('since',), ('morning',)]\n"
     ]
    }
   ],
   "source": [
    "val = [i for i in unigram]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unigram is same as tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'cat'), ('cat', 'is'), ('is', 'sitting'), ('sitting', 'with'), ('with', 'two'), ('two', 'puppies'), ('puppies', 'and'), ('and', 'playing'), ('playing', 'with'), ('with', 'them'), ('them', 'since'), ('since', 'morning')]\n"
     ]
    }
   ],
   "source": [
    "val = [i for i in bigram]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'cat', 'is'), ('cat', 'is', 'sitting'), ('is', 'sitting', 'with'), ('sitting', 'with', 'two'), ('with', 'two', 'puppies'), ('two', 'puppies', 'and'), ('puppies', 'and', 'playing'), ('and', 'playing', 'with'), ('playing', 'with', 'them'), ('with', 'them', 'since'), ('them', 'since', 'morning')]\n"
     ]
    }
   ],
   "source": [
    "val = [i for i in trigram]\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usecase: you want to learn something in sequence\n",
    "### In order to predict the next word you need to learn the sequence\n",
    "### you are learning the semantic meaning of the text\n",
    "### POS tagger not used now days: nobody is focusing on gramatical features nowdays, all focus on sequence of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `END --------------------------------------`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
